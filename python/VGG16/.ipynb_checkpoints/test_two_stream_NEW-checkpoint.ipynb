{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from accelerator import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "class CNN_accelerator(object):\n",
    "    hardware_instance = None\n",
    "        \n",
    "    def __init__(self, config_path=None, hardware_instance=None):\n",
    "        assert config_path is not None\n",
    "        assert hardware_instance is not None\n",
    "        \n",
    "        self.read_config(config)\n",
    "        \n",
    "        self.core0 = hardware_instance\n",
    "        \n",
    "        # allocate buffer for input image\n",
    "        self.input_buff = self.xlnk.cma_array(\n",
    "            shape=(self.buffer_depth, self.WORD_LENGTH),\n",
    "            dtype=np.uint8)\n",
    "            \n",
    "    def read_config(self, config_path):\n",
    "        config = configparser.ConfigParser()  \n",
    "        config.read(config_path)\n",
    "        \n",
    "        self.board = config[\"FPGAConfig\"][\"name\"]\n",
    "        self.bitstream_path = config[\"FPGAConfig\"][\"bitstream_path\"]\n",
    "        self.precision = int(config[\"PEConfig\"][\"precision\"])\n",
    "        self.data_width = int(config[\"PEConfig\"][\"data_width\"])\n",
    "        self.Ti = int(config[\"PEConfig\"][\"Ti\"])\n",
    "        self.To = int(config[\"PEConfig\"][\"To\"])\n",
    "        self.Tr = int(config[\"PEConfig\"][\"Tr\"])\n",
    "        self.Tc = int(config[\"PEConfig\"][\"Tc\"])\n",
    "\n",
    "        self.img_height = int(config[\"DataConfig\"][\"image_height\"])\n",
    "        self.img_width = int(config[\"DataConfig\"][\"image_width\"])\n",
    "        self.img_channel = int(config[\"DataConfig\"][\"image_channel\"])\n",
    "\n",
    "        self.WORD_LENGTH = int(ceil(self.data_width/self.precision))\n",
    "        self.buffer_depth = int(ceil((self.img_channel*self.img_height*self.img_width)/self.WORD_LENGTH))\n",
    "        print(\"Initialize configuration...: done\") \n",
    "        \n",
    "    def __call__(self, raw_image):\n",
    "        self._convert_raw_image_to_buffer(raw_image)\n",
    "        print(\"executing layers...\")\n",
    "\n",
    "        hw_begin = time.perf_counter()\n",
    "        fm = self.layers[0](self.input_buff)\n",
    "        print(\"layer {}, IFM size = {}, WGT size = {}, OFM size = {}, time = {}(ms)\"\\\n",
    "            .format(0, \\\n",
    "                (self.layers[0].in_height, self.layers[0].in_width, self.layers[0].in_channel),\\\n",
    "                self.layers[0].weight_shape, \\\n",
    "                (self.layers[0].out_height, self.layers[0].out_width, self.layers[0].out_channel),\\\n",
    "                (time.perf_counter() - hw_begin)*1000))\n",
    "\n",
    "        cnt = 1\n",
    "        for l in self.layers[1:]:\n",
    "            hw_begin = time.perf_counter()\n",
    "            fm = l(fm)\n",
    "            print(\"layer {}, IFM size = {}, WGT size = {}, OFM size = {}, time = {}(ms)\"\\\n",
    "            .format(cnt, \\\n",
    "                (l.in_height, l.in_width, l.in_channel),\\\n",
    "                l.weight_shape, \\\n",
    "                (l.out_height, l.out_width, l.out_channel),\\\n",
    "                (time.perf_counter() - hw_begin)*1000))\n",
    "            cnt+=1\n",
    "\n",
    "        return fm\n",
    "\n",
    "    def mem_alloc(self, out_channel, in_channel, in_height, in_width, ker):\n",
    "        print(\"memory allocation...\")\n",
    "        # ifm_depth = int(ceil((in_channel*in_height*in_width)/self.WORD_LENGTH))\n",
    "        if (out_channel % self.To != 0):\n",
    "            out_channel += self.To - (out_channel % self.To)\n",
    "\n",
    "        fm_depth = int(ceil((out_channel*in_height*in_width)/self.WORD_LENGTH))\n",
    "        wgt_depth = int(ceil((in_channel*out_channel*ker*ker)/self.WORD_LENGTH))\n",
    "\n",
    "        # ifm_buff = self.xlnk.cma_array(shape=(ifm_depth, self.WORD_LENGTH), dtype=np.uint8)\n",
    "        fm_buff = self.xlnk.cma_array(shape=(fm_depth, self.WORD_LENGTH), dtype=np.uint8)\n",
    "        wgt_buff = self.xlnk.cma_array(shape=(wgt_depth, self.WORD_LENGTH), dtype=np.uint8)\n",
    "        #print(\"memory allocation...: done\")\n",
    "\n",
    "        return fm_buff, wgt_buff\n",
    "\n",
    "    def setting(self, ofm_buff, ifm_buff, wgt_buff,\n",
    "                out_channel, in_channel, in_height, in_width, ker=3, s=1, poolWin=1):\n",
    "\n",
    "        self.core0.write(0x10, ifm_buff.physical_address)\n",
    "        self.core0.write(0x18, ofm_buff.physical_address)\n",
    "        self.core0.write(0x20, wgt_buff.physical_address)\n",
    "        self.core0.write(0x28, int(in_height))\n",
    "        self.core0.write(0x30, int(in_width))\n",
    "        self.core0.write(0x38, int(in_channel))\n",
    "        self.core0.write(0x40, int(out_channel))\n",
    "        self.core0.write(0x48, self.Tr)\n",
    "        self.core0.write(0x50, self.Tc)\n",
    "        self.core0.write(0x58, ker)\n",
    "        self.core0.write(0x60, s)\n",
    "        self.core0.write(0x68, poolWin)\n",
    "        \n",
    "    def execute(self):\n",
    "        self.core0.write(0x00, 1)\n",
    "        isready = self.core0.read(0x00)\n",
    "\n",
    "        while( isready == 1 ):\n",
    "            isready = self.core0.read(0x00)\n",
    "            \n",
    "    def load_parameters(self):\n",
    "        if self.layers is None:\n",
    "            raise(\"Network layers are not initialized\")\n",
    "\n",
    "        for l in self.layers:\n",
    "            if l.type is not \"conv\":\n",
    "                continue\n",
    "\n",
    "            if l.weight_data.shape != l.weight_shape:\n",
    "                raise(\"Input weight shape is \" + l.weight_data.shape \\\n",
    "                    + \", not match with setting \" + l.weight_shape)\n",
    "\n",
    "            self._convert_weight_to_buffer(l)\n",
    "\n",
    "        return 0\n",
    "    \n",
    "    def _convert_raw_image_to_buffer(self, raw_image):\n",
    "        imgH = raw_image.shape[0]\n",
    "        imgW = raw_image.shape[1]\n",
    "        img_channel = raw_image.shape[2]\n",
    "        \n",
    "        if img_channel < self.Ti:\n",
    "            zero_padding = np.zeros((imgH, imgW, self.Ti-img_channel), dtype = np.uint8)\n",
    "            raw_image = np.concatenate((raw_image, zero_padding), axis = 2)\n",
    "\n",
    "        np.copyto(self.input_buff, raw_image.reshape(-1,raw_image.shape[2]))\n",
    "        \n",
    "    \"\"\"\n",
    "    Convert pytorch WGT to Xlnk input\n",
    "    Input:\n",
    "        1. wgt: pytorch tensor(out channel, in channel, ker_height, ker_width)\n",
    "    Output:\n",
    "        1. wgt_cma: Xlnk cma(Depth, WORD_LENGTH), \n",
    "            Depth = (out_channel/To) * (in_channel/Ti) * To * ker_height * ker_width * (Ti/WORD_LENGTH) * WORD_LENGTH\n",
    "    \"\"\"\n",
    "    def _convert_weight_to_buffer(self, layer):\n",
    "        wgt = layer.weight_data\n",
    "        out_channel = layer.out_channel\n",
    "        in_channel = layer.in_channel\n",
    "        kerH = layer.ker\n",
    "        kerW = layer.ker\n",
    "\n",
    "        if in_channel < self.Ti:\n",
    "            zero_padding = np.zeros((out_channel,self.Ti - in_channel,kerH,kerW), dtype=np.uint8)\n",
    "            wgt = np.concatenate((wgt,zero_padding), axis = 1)\n",
    "\n",
    "        # if (out_channel % self.To != 0):\n",
    "        #   zero_padding = np.zeros((self.To - (out_channel % self.To),in_channel,kerH,kerW), dtype=np.uint8)\n",
    "        #   wgt = np.concatenate((wgt,zero_padding), axis = 0)\n",
    "\n",
    "        print(\"shape of wgt: \", wgt.shape)\n",
    "        wgt_tmp = np.transpose(\\\n",
    "            wgt.reshape((int(ceil(out_channel/self.To)), self.To, int(ceil(in_channel/self.Ti)), self.Ti, kerH, kerW)), \\\n",
    "                (0,2,1,4,5,3))\\\n",
    "                .reshape((int(ceil(out_channel/self.To)), int(ceil(in_channel/self.Ti)), self.To, kerH, kerW,int(self.Ti/self.WORD_LENGTH),self.WORD_LENGTH))\\\n",
    "                .reshape(-1,self.WORD_LENGTH)\n",
    "\n",
    "        np.copyto(layer.wgt_buff, wgt_tmp)\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fpga_nn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%microblaze/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#from accelerator import CNN_accelerator\n",
    "import conv_operation as co\n",
    "\n",
    "class Conv2D():\n",
    "    def __init__(self, out_channel, in_channel, in_height, in_width, ker=3, s=1, accelerator=None):\n",
    "        assert accelerator is not None\n",
    "\n",
    "        self.type = \"conv\"\n",
    "        self.out_channel = out_channel\n",
    "        self.in_channel = in_channel\n",
    "        self.in_height = in_height\n",
    "        self.in_width = in_width\n",
    "        self.out_height = in_height\n",
    "        self.out_width = in_width\n",
    "        self.ker = ker\n",
    "        self.s = s\n",
    "        self.weight_shape = (out_channel, in_channel, ker, ker)\n",
    "        self.weight_data = None\n",
    "        \n",
    "        self.accelerator = accelerator\n",
    "        \n",
    "        self.ofm_buff, self.wgt_buff = \\\n",
    "            self.accelerator.mem_alloc(max(out_channel, self.To)\\\n",
    "            , max(in_channel, self.Ti), self.out_height, self.out_width, ker)\n",
    "\n",
    "    def __call__(self, ifm_buff):\n",
    "        print(\"executing conv2d\")\n",
    "        self.accelerator.ifm_buff = ifm_buff\n",
    "        self.accelerator.setting(self.ofm_buff,\\\n",
    "                                 self.ifm_buff,\\\n",
    "                                 self.wgt_buff,\\\n",
    "                                 self.out_channel,\\\n",
    "                                 self.in_channel,\\\n",
    "                                 self.in_height,\\\n",
    "                                 self.in_width,\\\n",
    "                                 self.ker,\\\n",
    "                                 self.s)\n",
    "        self.accelerator.execute()\n",
    "        return self.accelerator.ofm_buff\n",
    "\n",
    "class Conv2DPool():\n",
    "    def __init__(self, out_channel, in_channel, in_height, in_width, ker=3, s=1, poolWin = 2, accelerator=None):\n",
    "        assert accelerator is not None\n",
    "\n",
    "        self.type = \"conv\"\n",
    "        self.out_channel = out_channel\n",
    "        self.in_channel = in_channel\n",
    "        self.in_height = in_height\n",
    "        self.in_width = in_width\n",
    "        self.out_height = int(ceil(in_height/poolWin))\n",
    "        self.out_width = int(ceil(in_width/poolWin))\n",
    "        self.ker = ker\n",
    "        self.s = s\n",
    "        self.poolWin = poolWin\n",
    "        self.weight_shape = (out_channel, in_channel, ker, ker)\n",
    "        self.weight_data = None\n",
    "        \n",
    "        self.accelerator = accelerator\n",
    "        \n",
    "        self.ofm_buff, self.wgt_buff = \\\n",
    "            self.accelerator.mem_alloc(max(out_channel, self.To)\\\n",
    "                , max(in_channel, self.Ti), self.out_height, self.out_width, ker)\n",
    "\n",
    "    def __call__(self, ifm_buff):\n",
    "        print(\"executing conv2dPool\")\n",
    "        self.accelerator.ifm_buff = ifm_buff\n",
    "        self.accelerator.setting(self.ofm_buff,\\\n",
    "                                 self.ifm_buff,\\\n",
    "                                 self.wgt_buff,\\\n",
    "                                 self.out_channel,\\\n",
    "                                 self.in_channel,\\\n",
    "                                 self.in_height,\\\n",
    "                                 self.in_width,\\\n",
    "                                 poolWin = self.poolWin)\n",
    "        self.accelerator.execute()\n",
    "        return self.accelerator.ofm_buff\n",
    "    \n",
    "class Linear():\n",
    "    def __init__(self, out_channel, in_channel):\n",
    "        super(Linear, self).__init__()\n",
    "\n",
    "        self.type = \"linear\"\n",
    "        self.out_channel = out_channel\n",
    "        self.in_channel = in_channel\n",
    "        self.in_height = 1\n",
    "        self.in_width = 1\n",
    "        self.out_height = 1\n",
    "        self.out_width = 1\n",
    "        self.ker = 1\n",
    "        self.weight_shape = (out_channel, in_channel, 1, 1)\n",
    "        self.weight_data = None\n",
    "\n",
    "        # self.ofm_buff, self.wgt_buff = \\\n",
    "        # self.mem_alloc(max(out_channel, self.To)\\\n",
    "        #   , max(in_channel, self.Ti), self.out_height, self.out_width, self.ker)\n",
    "\n",
    "    def __call__(self, feature):\n",
    "        print(\"executing Fully Connected Layer\")\n",
    "        # feature: (1, in_channel * in_height * in_width)\n",
    "        # wgt: (out_channel, in_channel * in_height * in_width)\n",
    "        return co.sw_linear(feature, self.weight_data)\n",
    "\n",
    "class Flatten():\n",
    "    def __init__(self, in_height, in_width, in_channel):\n",
    "        super(Flatten, self).__init__()\n",
    "        \n",
    "        self.type = \"flatten\"\n",
    "        self.in_height = in_height\n",
    "        self.in_width = in_width\n",
    "        self.in_channel = in_channel\n",
    "\n",
    "        self.out_height = 1\n",
    "        self.out_width = 1\n",
    "        self.out_channel = self.in_height * self.in_width * self.in_channel\n",
    "\n",
    "        self.weight_shape = (in_height, in_width, in_channel)\n",
    "        # self.tile_depth = int(in_height*in_width)\n",
    "\n",
    "    def __call__(self, feature_buff):\n",
    "        print(\"executing Flatten\")\n",
    "        # convert to row major feature map(channel, height, width)\n",
    "        return co.convertOFMOutput(\\\n",
    "                feature_buff, feature_buff.shape[0], self.WORD_LENGTH\\\n",
    "                , self.in_channel, self.in_height, self.in_width, Ti = self.Ti)\\\n",
    "            .reshape((self.in_channel * self.in_height * self.in_width, -1))\n",
    "\n",
    "        # buffer_depth = hw_buffer.shape[0]\n",
    "\n",
    "        # # TODO: padding zero\n",
    "        # num_tile = int(ceil(buffer_depth/self.tile_depth))\n",
    "        # return np.transpose(\\\n",
    "        #   hw_buffer.reshape((num_tile,self.tile_depth,self.WORD_LENGTH)),(0,2,1))\\\n",
    "        # .reshape((buffer_depth,self.WORD_LENGTH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## two_stream.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-e8d8373282d8>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-e8d8373282d8>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    configa = xx xx\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "\n",
    "import numpy as np\n",
    "import fpga_nn\n",
    "\n",
    "def make_layers(config_path=None, in_channel=3, accelerator=None):\n",
    "    assert config_path is not None\n",
    "    assert accelerator is not None\n",
    "    \n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_path)\n",
    "\n",
    "    in_height = int(config[\"DataConfig\"][\"image_height\"])\n",
    "    in_width = int(config[\"DataConfig\"][\"image_width\"])\n",
    "    in_channel = in_channel\n",
    "\n",
    "    layers = []\n",
    "    # Conv(output channel, input channel, input height, input width, kerSize, stride)\n",
    "    layers += [fpga_nn.Conv2D(64, in_channel, in_height, in_width, ker = 3, s = 1, accelerator)]\n",
    "    layers += [fpga_nn.Conv2DPool(128, 64, in_height, in_width, ker = 3, poolWin = 2, accelerator)]\n",
    "\n",
    "    layers += [fpga_nn.Conv2D(128, 128, int(in_height/2), int(in_width/2), ker = 3, s = 1, accelerator)]\n",
    "    layers += [fpga_nn.Conv2DPool(256, 128, int(in_height/2), int(in_width/2), ker = 3, poolWin = 2, accelerator)]\n",
    "\n",
    "    layers += [fpga_nn.Conv2D(256, 256, int(in_height/4), int(in_width/4), ker = 3, s = 1, accelerator)]\n",
    "    layers += [fpga_nn.Conv2D(256, 256, int(in_height/4), int(in_width/4), ker = 3, s = 1, accelerator)]\n",
    "    layers += [fpga_nn.Conv2DPool(512, 256, int(in_height/4), int(in_width/4), ker = 3, poolWin = 2, accelerator)]\n",
    "\n",
    "    layers += [fpga_nn.Conv2D(512, 512, int(in_height/8), int(in_width/8), ker = 3, s = 1, accelerator)]\n",
    "    layers += [fpga_nn.Conv2D(512, 512, int(in_height/8), int(in_width/8), ker = 3, s = 1, accelerator)]\n",
    "    layers += [fpga_nn.Conv2DPool(512, 512, int(in_height/8), int(in_width/8), ker = 3, poolWin = 2, accelerator)]\n",
    "\n",
    "    layers += [fpga_nn.Conv2D(512, 512, int(in_height/16), int(in_width/16), ker = 3, s = 1, accelerator)]\n",
    "    layers += [fpga_nn.Conv2D(512, 512, int(in_height/16), int(in_width/16), ker = 3, s = 1, accelerator)]\n",
    "    layers += [fpga_nn.Conv2DPool(512, 512, int(in_height/16), int(in_width/16), ker = 3, poolWin = 2, accelerator)]\n",
    "\n",
    "    # conv output size = (8,8,512)\n",
    "    layers += [fpga_nn.Flatten(int(in_height/32), int(in_width/32), 512)]\n",
    "    layers += [fpga_nn.Linear(4096,int(in_height/32)*int(in_width/32)*512)]\n",
    "    layers += [fpga_nn.Linear(101,4096)]\n",
    "\n",
    "    return layers\n",
    "\n",
    "class SimpleNet(fpga_nn.Module):\n",
    "    def __init__(self, layers, params_path = None):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.layers = layers\n",
    "        self.params_path = params_path\n",
    "\n",
    "        # initialize weight for each layer\n",
    "        self._init_weight()\n",
    "\n",
    "        # copy weight data to hardware buffer \n",
    "        self.load_parameters();\n",
    "\n",
    "    # TODO: load from parameter file\n",
    "    def _init_weight(self, params_path = None):\n",
    "        for l in self.layers:\n",
    "            if l.type == \"conv\" or l.type == \"linear\":\n",
    "                l.weight_data = np.random.randint(256,size=l.weight_shape, dtype=np.uint8)\n",
    "                \n",
    "def simple_net():\n",
    "    layers = make_layers()\n",
    "    params_path = \"params.path\"\n",
    "    model = SimpleNet(layers, params_path = params_path)\n",
    "    return model\n",
    "\n",
    "def simple_net_2():\n",
    "    layers = make_layers(in_channel=32)\n",
    "    params_path = \"params.path\"\n",
    "    model = SimpleNet(layers, params_path = params_path)\n",
    "    return model\n",
    "\n",
    "from pynq import Overlay\n",
    "from pynq import Xlnk\n",
    "\n",
    "class Two_stream(object):\n",
    "    def __init__(self, bitstr):\n",
    "        overlay = Overlay('../files/design_1.bit')\n",
    "        \n",
    "        lucas_kanade_acc = LK_acc(overlay)\n",
    "        \n",
    "        configa = xx xx\n",
    "        model_tmp_acc = VGG_accelerator(configa)\n",
    "        \n",
    "        configa = xx xx\n",
    "        model_spa_acc = VGG_accelerator(configb)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
